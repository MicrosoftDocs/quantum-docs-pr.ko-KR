---
title: 퀀텀 machine learning 라이브러리 용어집
description: 퀀텀 machine learning 약관 용어집
author: alexeib2
ms.author: alexeib
ms.date: 2/27/2020
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.training
no-loc:
- Q#
- $$v
ms.openlocfilehash: 39974af0121a5167f1965e508cd595535178548b
ms.sourcegitcommit: 9b0d1ffc8752334bd6145457a826505cc31fa27a
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 09/21/2020
ms.locfileid: "90833912"
---
# <a name="quantum-machine-learning-glossary"></a><span data-ttu-id="06af3-103">퀀텀 Machine Learning 용어집</span><span class="sxs-lookup"><span data-stu-id="06af3-103">Quantum Machine Learning glossary</span></span>

<span data-ttu-id="06af3-104">회로 중심 퀀텀 분류자의 교육은 기존 분류자의 교육으로 서 평가판 및 오류에의 한 동일한 (또는 약간 더 큰)의 보정을 요구 하는 많은 이동 부분이 포함 된 프로세스입니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-104">Training of a circuit-centric quantum classifier is a process with many moving parts that require the same (or slightly larger) amount of calibration by trial and error as training of traditional classifiers.</span></span> <span data-ttu-id="06af3-105">여기서는이 학습 프로세스의 주요 개념 및 원료를 정의 합니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-105">Here we define the main concepts and ingredients of this training process.</span></span>

## <a name="trainingtesting-schedules"></a><span data-ttu-id="06af3-106">교육/테스트 일정</span><span class="sxs-lookup"><span data-stu-id="06af3-106">Training/testing schedules</span></span>

<span data-ttu-id="06af3-107">분류자의 컨텍스트에서 *일정* 은 전체 학습 또는 테스트 집합의 데이터 샘플 하위 집합을 설명 합니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-107">In the context of classifier training a *schedule* describes a subset of data samples in an overall training or testing set.</span></span> <span data-ttu-id="06af3-108">일정은 일반적으로 샘플 인덱스의 컬렉션으로 정의 됩니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-108">A schedule is usually defined as a collection of sample indices.</span></span>

## <a name="parameterbias-scores"></a><span data-ttu-id="06af3-109">매개 변수/바이어스 점수</span><span class="sxs-lookup"><span data-stu-id="06af3-109">Parameter/bias scores</span></span>

<span data-ttu-id="06af3-110">후보 매개 변수 벡터와 분류자 바이어스가 지정 된 경우 *유효성 검사 점수* 는 선택한 유효성 검사 일정을 기준으로 측정 되며 일정의 모든 샘플에 대해 계산 된 많은 수의 오 분류로 표시 됩니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-110">Given a candidate parameter vector and a classifier bias, their *validation score* is measured relative to a chosen validation schedule S and is expressed by a number of misclassifications counted over all the samples in the schedule S.</span></span>

## <a name="hyperparameters"></a><span data-ttu-id="06af3-111">하이퍼 매개 변수</span><span class="sxs-lookup"><span data-stu-id="06af3-111">Hyperparameters</span></span>

<span data-ttu-id="06af3-112">모델 학습 프로세스는 하이퍼 *매개 변수*라고 하는 특정 미리 설정 된 값의 적용을 받습니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-112">The model training process is governed by certain pre-set values called *hyperparameters*:</span></span>

### <a name="learning-rate"></a><span data-ttu-id="06af3-113">학습 속도</span><span class="sxs-lookup"><span data-stu-id="06af3-113">Learning rate</span></span>

<span data-ttu-id="06af3-114">키 하이퍼 매개 변수 중 하나입니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-114">It is one of the key hyperparameters.</span></span> <span data-ttu-id="06af3-115">이는 매개 변수 업데이트에 영향을 주는 현재 추계 그라데이션 예상 크기를 정의 합니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-115">It defines how much current stochastic gradient estimate impacts the parameter update.</span></span> <span data-ttu-id="06af3-116">매개 변수 업데이트 델타의 크기는 학습 률에 비례 합니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-116">The size of parameter update delta is proportional to the learning rate.</span></span> <span data-ttu-id="06af3-117">더 작은 학습 속도 값은 더 느린 매개 변수 진화와 느린 수렴으로 이어질 수 있지만, 대부분의 경우에는 값이 너무 크면 일치를 중단할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-117">Smaller learning rate values lead to slower parameter evolution and slower convergence, but excessively large values of LR may break the convergence altogether as the gradient descent never commits to a particular local minimum.</span></span> <span data-ttu-id="06af3-118">학습 률은 학습 알고리즘에 따라 어느 정도까지 적절 하 게 조정 되지만 좋은 초기 값을 선택 하는 것이 중요 합니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-118">While learning rate is adaptively adjusted by the training algorithm to some extent, selecting a good initial value for it is important.</span></span> <span data-ttu-id="06af3-119">학습 요금에 대 한 일반적인 기본 초기 값은 0.1입니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-119">A usual default initial value for learning rate is 0.1.</span></span> <span data-ttu-id="06af3-120">학습 률의 가장 적합 한 값을 선택 하는 것은 자세한 그림입니다 (예: Goodfellow et의 섹션 4.3, "심층 학습", MIT 누르기, 2017).</span><span class="sxs-lookup"><span data-stu-id="06af3-120">Selecting the best value of learning rate is a fine art (see, for example, section 4.3 of Goodfellow et al.,"Deep learning", MIT Press, 2017).</span></span>

### <a name="minibatch-size"></a><span data-ttu-id="06af3-121">미니 배치 크기</span><span class="sxs-lookup"><span data-stu-id="06af3-121">Minibatch size</span></span>

<span data-ttu-id="06af3-122">추계 그라디언트의 단일 예측에 사용 되는 데이터 샘플 수를 정의 합니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-122">Defines how many data samples is used for a single estimation of stochastic gradient.</span></span> <span data-ttu-id="06af3-123">미니 배치 크기의 값이 클수록 일반적으로 더 강력 하 고 단조가 증가 하지만, minimatch 크기에 비례 하는 한 가지 그라데이션 추정의 비용이 발생 하기 때문에 학습 프로세스의 속도를 저하 시킬 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-123">Larger values of minibatch size generally lead to more robust and more monotonic convergence but can potentially slow down the training process, as the cost of any one gradient estimation is proportional to the minimatch size.</span></span> <span data-ttu-id="06af3-124">미니 배치 크기의 일반적인 기본값은 10입니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-124">A usual default value for the minibatch size is 10.</span></span>

### <a name="training-epochs-tolerance-gridlocks"></a><span data-ttu-id="06af3-125">교육 epoch, 허용 오차, gridlocks</span><span class="sxs-lookup"><span data-stu-id="06af3-125">Training epochs, tolerance, gridlocks</span></span>

<span data-ttu-id="06af3-126">"Epoch"는 예약 된 학습 데이터를 통과 하는 하나의 완전 한 과정을 의미 합니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-126">"Epoch" means one complete pass through the scheduled training data.</span></span>
<span data-ttu-id="06af3-127">학습 스레드 (아래 참조) 당 최대 epoch 수는 작아야 합니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-127">The maximum number of epochs per a training thread (see below) should be capped.</span></span> <span data-ttu-id="06af3-128">최대 개수의 epoch가 실행 될 때 학습 스레드는 종료 되도록 정의 됩니다 (가장 알려진 후보 매개 변수 포함).</span><span class="sxs-lookup"><span data-stu-id="06af3-128">The training thread is defined to terminate (with the best known candidate parameters) when the maximum number of epochs has been run.</span></span> <span data-ttu-id="06af3-129">그러나 유효성 검사 일정에 대 한 잘못 된 분류 비율이 선택한 허용 오차 미만으로 떨어지면 이러한 교육은 이전에 종료 됩니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-129">However such training would terminate earlier when misclassification rate on validation schedule falls below a chosen tolerance.</span></span> <span data-ttu-id="06af3-130">예를 들어 잘못 된 분류 허용 오차는 0.01 (1%)입니다. 2000의 유효성 검사 집합에서 20 개 미만의 분류를 표시 하는 경우 허용 오차 수준이 달성 됩니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-130">Suppose, for example, that misclassification tolerance is 0.01 (1%); if on validation set of 2000 samples we are seeing fewer than 20 misclassifications, then the tolerance level has been achieved.</span></span> <span data-ttu-id="06af3-131">또한 후보 모델의 유효성 검사 점수가 여러 연속 epoch (gridlock)에 대해 향상 된 것으로 표시 되지 않은 경우 학습 스레드는 중간에 종료 됩니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-131">A training thread also terminates prematurely if the validation score of the candidate model has not shown any improvement over several consecutive epochs (a gridlock).</span></span> <span data-ttu-id="06af3-132">Gridlock 종료에 대 한 논리는 현재 하드 코드 되어 있습니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-132">The logic for the gridlock termination is currently hardcoded.</span></span>

### <a name="measurements-count"></a><span data-ttu-id="06af3-133">측정값 수</span><span class="sxs-lookup"><span data-stu-id="06af3-133">Measurements count</span></span>

<span data-ttu-id="06af3-134">퀀텀 장치에서 추계 그라데이션의 구성 요소 및 학습/유효성 검사 점수를 예측 하 여 적절 한 관찰 가능 개체의 여러 측정이 필요한 퀀텀 상태 겹침을 예측 합니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-134">Estimating the training/validation scores and the components of the stochastic gradient on a quantum device amounts to estimating quantum state overlaps that requires multiple measurements of the appropriate observables.</span></span> <span data-ttu-id="06af3-135">측정 수는 $O (1/\ 엡실론 ^ 2)로 확장 해야 합니다. 여기서 $ \epsilon $은 원하는 추정 오류입니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-135">The number of measurements should scale as $O(1/\epsilon^2)$ where $\epsilon$ is the desired estimation error.</span></span>
<span data-ttu-id="06af3-136">이에 대 한 규칙에 따라 초기 측정 수는 약 $1/\ mbox {tolerance} ^ 2 $ 일 수 있습니다 (이전 단락의 허용 오차 정의 참조).</span><span class="sxs-lookup"><span data-stu-id="06af3-136">As a rule of thumb, the initial measurements count could be approximately $1/\mbox{tolerance}^2$ (see definition of tolerance in the previous paragraph).</span></span> <span data-ttu-id="06af3-137">그라데이션 디센더가 너무 불안정 하 여 너무 많이 수렴 하 여 달성할 수 없는 것 처럼 보이는 경우에는 측정값 수를 위쪽으로 수정 해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-137">One would need to revise the measurement count upward if the gradient descent appears to be too erratic and convergence too hard to achieve.</span></span>

### <a name="training-threads"></a><span data-ttu-id="06af3-138">스레드 학습</span><span class="sxs-lookup"><span data-stu-id="06af3-138">Training threads</span></span>

<span data-ttu-id="06af3-139">분류자의 학습 유틸리티인 가능성 함수는 거의 발생 하지 않습니다. 즉, 일반적으로 품질에 따라 크게 다를 수 있는 매개 변수 공간에는 다 수의 로컬 optima 있습니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-139">The likelihood function which is the training utility for the classifier is very seldom convex, meaning that it usually has a multitude of local optima in the parameter space that may differ significantly by quality.</span></span> <span data-ttu-id="06af3-140">SGD 프로세스는 하나의 특정 최적에만 수렴 될 수 있으므로 여러 개의 시작 매개 변수 벡터를 탐색 하는 것이 중요 합니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-140">Since the SGD process can converge to only one specific optimum, it is important to explore multiple starting parameter vectors.</span></span> <span data-ttu-id="06af3-141">기계 학습에서 일반적인 방법은 이러한 시작 벡터를 임의로 초기화 하는 것입니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-141">Common practice in machine learning is to initialize such starting vectors randomly.</span></span> <span data-ttu-id="06af3-142">Q#학습 API는 이러한 시작 벡터의 임의 배열을 허용 하지만 기본 코드는 순차적으로 탐색 합니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-142">The Q# training API accepts an arbitrary array of such starting vectors but the underlying code explores them sequentially.</span></span> <span data-ttu-id="06af3-143">다중 코어 컴퓨터에서 또는 병렬 컴퓨팅 아키텍처의 경우 호출에서 여러 Q# 매개 변수 초기화를 사용 하 여 동시에 학습 API를 여러 번 호출 하는 것이 좋습니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-143">On a multicore computer or in fact on any parallel computing architecture it is advisable to perform several calls to Q# training API in parallel with different parameter initializations across the calls.</span></span>

#### <a name="how-to-modify-the-hyperparameters"></a><span data-ttu-id="06af3-144">하이퍼 매개 변수를 수정 하는 방법</span><span class="sxs-lookup"><span data-stu-id="06af3-144">How to modify the hyperparameters</span></span>

<span data-ttu-id="06af3-145">QML 라이브러리에서 하이퍼 매개 변수를 수정 하는 가장 좋은 방법은 UDT의 기본값을 재정의 하는 것입니다 [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions) .</span><span class="sxs-lookup"><span data-stu-id="06af3-145">In the QML library, the best way to modify the hyperparameters is by overriding the default values of the UDT [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions).</span></span> <span data-ttu-id="06af3-146">이렇게 하려면 함수를 사용 하 여 호출 하 [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) 고 연산자를 적용 `w/` 하 여 기본값을 재정의 합니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-146">To do this we call it with the function [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) and apply the operator `w/` to override the default values.</span></span> <span data-ttu-id="06af3-147">예를 들어 10만 측정치와 0.01의 학습 률을 사용 하려면 다음을 수행 합니다.</span><span class="sxs-lookup"><span data-stu-id="06af3-147">For example, to use 100,000 measurements and a learning rate of 0.01:</span></span>
 ```qsharp
let options = DefaultTrainingOptions()
w/ LearningRate <- 0.01
w/ NMeasurements <- 100000;
 ```
