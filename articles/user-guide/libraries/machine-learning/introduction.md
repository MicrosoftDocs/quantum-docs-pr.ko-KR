---
title: 양자 기계 학습 라이브러리
author: alexeib2
ms.author: alexei.bocharov@microsoft.com
ms.date: 11/22/2019
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.introduction
ms.openlocfilehash: 4c42612fee3a58e15368677bb2c77a70c5680f45
ms.sourcegitcommit: 0181e7c9e98f9af30ea32d3cd8e7e5e30257a4dc
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 06/23/2020
ms.locfileid: "85276176"
---
# <a name="introduction-to-quantum-machine-learning"></a><span data-ttu-id="2b281-102">퀀텀 Machine Learning 소개</span><span class="sxs-lookup"><span data-stu-id="2b281-102">Introduction to Quantum Machine Learning</span></span>

## <a name="framework-and-goals"></a><span data-ttu-id="2b281-103">프레임 워크 및 목표</span><span class="sxs-lookup"><span data-stu-id="2b281-103">Framework and goals</span></span>

<span data-ttu-id="2b281-104">퀀텀 인코딩 및 정보 처리는 기존 machine learning 퀀텀 분류자에 대 한 강력한 대안입니다. 특히, 기능 수에 따라 간결한 형식의 퀀텀 레지스터에 데이터를 인코딩하고, 양자를 계산 리소스로 체계적으로 사용 하 고, 클래스 유추를 위한 퀀텀 측정을 적용 하는 것이 좋습니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-104">Quantum encoding and processing of information is a powerful alternative to classical machine learning Quantum classifiers, in particular, encode data in quantum registers that are concise relative to the number of features, systematically employ quantum entanglement as computational resource and employ quantum measurement for class inference.</span></span>
<span data-ttu-id="2b281-105">회로 중심 퀀텀 분류자는 데이터 인코딩을 빠르게 entangling/disentangling 퀀텀 회로와 결합 하 여 데이터 예제의 클래스 레이블을 유추 하는 비교적 간단한 퀀텀 솔루션입니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-105">Circuit centric quantum classifier is a relatively simple quantum solution that combines data encoding with a rapidly entangling/disentangling quantum circuit followed by measurement to infer class labels of data samples.</span></span>
<span data-ttu-id="2b281-106">목표는 매우 큰 기능 공간에 대해서도 회로 매개 변수의 하이브리드 퀀텀/클래식 교육 뿐만 아니라 주체 회로의 기존 특성 및 저장소를 보장 하는 것입니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-106">The goal is to ensure classical characterization and storage of subject circuits, as well as hybrid quantum/classical training of the circuit parameters even for extremely large feature spaces.</span></span>

## <a name="classifier-architecture"></a><span data-ttu-id="2b281-107">분류자 아키텍처</span><span class="sxs-lookup"><span data-stu-id="2b281-107">Classifier architecture</span></span>

<span data-ttu-id="2b281-108">분류는 감독 된 기계 학습 작업으로, \{ \} 특정 데이터 샘플의 클래스 레이블 $ y_1, y_2, \ldots, y_d $를 유추 하는 것입니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-108">Classification is a supervised machine learning task, where the goal is to infer class labels $\{y_1,y_2,\ldots,y_d\}$ of certain data samples.</span></span> <span data-ttu-id="2b281-109">"학습 데이터 집합"은 \{ 미리 할당 된 알려진 레이블이 있는 samples $ \mathcal{D} = (x, y)} $의 컬렉션입니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-109">The "training data set" is a collection of samples $\mathcal{D}=\{(x,y)}$ with known pre-assigned labels.</span></span> <span data-ttu-id="2b281-110">$X 여기에서 $는 데이터 샘플 이며 $y $는 알려진 레이블 "학습 레이블"입니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-110">Here $x$ is a data sample and $y$ is its known label called "training label".</span></span>
<span data-ttu-id="2b281-111">기존 메서드와 약간 비슷하며 퀀텀 분류는 다음 세 단계로 구성 됩니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-111">Somewhat similar to traditional methods, quantum classification consists of three steps:</span></span>
- <span data-ttu-id="2b281-112">데이터 인코딩</span><span class="sxs-lookup"><span data-stu-id="2b281-112">data encoding</span></span>
- <span data-ttu-id="2b281-113">분류자 상태 준비</span><span class="sxs-lookup"><span data-stu-id="2b281-113">preparation of a classifier state</span></span>
- <span data-ttu-id="2b281-114">측정 확률 측정의 특성으로 인해 이러한 세 단계를 여러 번 반복 해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-114">measurement Due to the probabilistic nature of the measurement, these three steps must be repeated multiple times.</span></span> <span data-ttu-id="2b281-115">측정은 비선형 활성화에 해당 하는 퀀텀으로 볼 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-115">The measurement may be viewed as a quantum equivalent of non-linear activation.</span></span>
<span data-ttu-id="2b281-116">분류자 상태의 인코딩과 컴퓨팅은 모두 *퀀텀 회로*를 통해 수행 됩니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-116">Both the encoding and the computing of the classifier state are done by means of *quantum circuits*.</span></span> <span data-ttu-id="2b281-117">인코딩 회로는 일반적으로 데이터를 기반으로 하 고 매개 변수를 사용 하지 않는 반면 분류자 회로는 충분 한 learnable 매개 변수 집합을 포함 합니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-117">While the encoding circuit is usually data-driven and parameter-free, the classifier circuit contains a sufficient set of learnable parameters.</span></span> 

<span data-ttu-id="2b281-118">제안 된 솔루션에서 분류자 회로는 단일의 비트 회전 및 2 중 비트 제어 회전으로 구성 됩니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-118">In the proposed solution the classifier circuit is composed of single-qubit rotations and two-qubit controlled rotations.</span></span> <span data-ttu-id="2b281-119">여기서 learnable 매개 변수는 회전 각도입니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-119">The learnable parameters here are the rotation angles.</span></span> <span data-ttu-id="2b281-120">회전 및 제어 된 회전 게이트는 퀀텀 계산에 대해 *universal* 이라고 하며,이는 모든 단일 무게 매트릭스가 이러한 게이트로 구성 된 길고 충분 한 회로로 분해할 수 있음을 의미 합니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-120">The rotation and controlled rotation gates are known to be *universal* for quantum computation, which means that any unitary weight matrix can be decomposed into a long enough circuit consisting of such gates.</span></span>

![다중 계층 퍼셉트론 및 회로 중심의 분류자](~/media/DLvsQCC.png)

<span data-ttu-id="2b281-122">기본 구조를 더 잘 이해 하기 위해이 모델을 다중 계층 퍼셉트론과 비교할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-122">We can compare this model to a multilayer perceptron to get a better understanding of the basic structure.</span></span> <span data-ttu-id="2b281-123">퍼셉트론에서 예측 $p (y | x, \theta) $는 비 선형 활성화 함수 (neurons)를 연결 하는 선형 함수를 결정 하는 가중치 $ \theta $로 매개 변수가 있는 됩니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-123">In the perceptron the predictor $p(y|x, \theta)$ is parametrized by the set of weights $\theta$ that determine the linear functions connecting the non-linear activation functions (neurons).</span></span> <span data-ttu-id="2b281-124">이러한 매개 변수를 학습 하 여 모델을 만들 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-124">These parameters can be trained to create the model.</span></span> <span data-ttu-id="2b281-125">출력 계층에서 소프트 max와 같은 비선형 활성화 함수를 사용 하 여 클래스에 속한 샘플의 확률을 얻을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-125">At the output layer we can get the probability of a sample belonging to a class by using non-linear activation functions like softmax.</span></span> <span data-ttu-id="2b281-126">회로 중심 분류자에서 예측은 모델 회로의 단선 비트 및 2-매개 변수가 있는 비트 제어 회전의 회전 각도에 의해 조정 됩니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-126">In the circuit centric classifier the predictor is parametrized by the rotation angles of the single-qubit and two-qubit controlled rotations of the model circuit.</span></span> <span data-ttu-id="2b281-127">이와 비슷한 방식으로 이러한 매개 변수는 그라데이션 디센더 알고리즘의 하이브리드 퀀텀/클래식 버전에서 학습 될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-127">In a similar fashion, those parameters can be trained by an hybrid quantum/classical version of the gradient descent algorithm.</span></span> <span data-ttu-id="2b281-128">선형이 아닌 활성화 함수를 사용 하는 대신 출력을 계산 하기 위해 제어 되는 회전 후 특정 비트에 대해 반복 측정을 읽어 클래스의 확률을 가져옵니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-128">To calculate the output, instead of using non-linear activation functions, the probability of the class is obtained by reading repeated measurements over a specific qubit after the controlled rotations.</span></span> <span data-ttu-id="2b281-129">퀀텀 상태에서 기존 데이터를 인코딩하려면 상태 준비를 위해 제어 가능한 인코딩 회로를 사용 합니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-129">To encode the classical data in a quantum state we use a controllable encoding circuit for state preparation.</span></span>

<span data-ttu-id="2b281-130">아키텍처는 상대적으로 얕은 회로를 탐색 하므로 모든 범위에서 데이터 기능 간의 상관 관계를 모두 캡처하기 위해 신속 하 게 *entangling* 해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-130">Our architecture explores relatively shallow circuits, which therefore must be *rapidly entangling* in order to capture all the correlations between the data features at all ranges.</span></span> <span data-ttu-id="2b281-131">가장 유용한 빠른 entangling 회로 구성 요소의 예는 아래 그림에 나와 있습니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-131">An example of the most useful rapidly entangling circuit component is shown on figure below.</span></span> <span data-ttu-id="2b281-132">이 geometry를 사용 하는 회로가 $3 n + 1 $ 게이트로만 구성 된 경우에도 계산 되는 단일 무게 매트릭스가 $2 ^ n $ 기능 간에 상당한 상호 통신할 수 있도록 합니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-132">Even though a circuit with this geometry consists of only $3 n+1$ gates, the unitary weight matrix that it computes ensures significant cross-talk between $2^n$ features.</span></span>

![5 개의 entangling에서 퀀텀 회로를 빠르게 하 고 두 개의 순환 계층을 사용 합니다.](~/media/5-qubit-qccc.png)

<span data-ttu-id="2b281-134">위 예제의 회로는 6 개의 단일 수준 게이트 $ (G_1, \ldots, G_5;로 구성 됩니다. G_ {16} ) $ 및 10 2-stbits 게이트 $ (G_6, \ldots, G_ {15} ) $입니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-134">The circuit in the above example consists of 6 single-qubit gates $(G_1,\ldots,G_5; G_{16})$ and 10 two-qubits gates $(G_6,\ldots,G_{15})$.</span></span> <span data-ttu-id="2b281-135">각 게이트가 하나의 learnable 매개 변수를 사용 하 여 정의 되는 것으로 가정 하면 learnable 매개 변수는 16 개 있으며, 5-bit 힐베르트 공간의 차원은 32입니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-135">Assuming that each of the gates is defined with one learnable parameter we have 16 learnable parameters, while the dimension of the 5-qubit Hilbert space is 32.</span></span> <span data-ttu-id="2b281-136">$2 ^ n $ 차원 기능 공간에 대 한 $3 n + 1 $ 매개 변수를 사용 하 여 회로를 생성 하 $n $가 홀수 인 경우 이러한 회로 기 하 도형은 $n $-ombit 레지스터에 쉽게 일반화 될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-136">Such circuit geometry can be easily generalized to any $n$-qubit register, when $n$ is odd, yielding circuits with $3 n+1$ parameters for $2^n$-dimensional feature space.</span></span>

## <a name="classifier-training-as-a-supervised-learning-task"></a><span data-ttu-id="2b281-137">감독 된 학습 작업으로 서의 분류자 학습</span><span class="sxs-lookup"><span data-stu-id="2b281-137">Classifier training as a supervised learning task</span></span>

<span data-ttu-id="2b281-138">분류자 모델의 교육은 작업 매개 변수의 최적 값을 찾기 때문에 학습 샘플에서 올바른 학습 레이블을 유추 하는 경우의 평균 가능성을 최대화 합니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-138">Training of a classifier model involves finding optimal values of its operational parameters, such that they maximize the average likelihood of inferring the correct training labels across the training samples.</span></span>
<span data-ttu-id="2b281-139">여기서는 두 가지 수준 분류만 사용 합니다. 즉, $d = $2의 경우와 레이블이 $y _1 인 두 개의 클래스 (y_2 $)만 고려 합니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-139">Here, we concern ourselves with two level classification only, i.e. the case of $d=2$ and only two classes with the labels $y_1,y_2$.</span></span>

> [!NOTE]
> <span data-ttu-id="2b281-140">메서드를 임의 수의 클래스로 일반화 하는 확실 방법은 qudits를 사용 하 여로 대체 하는 것입니다. 예를 들어, $ 상태를 $d 사용 하는 퀀텀 단위와 $d $ 방향 측정이 있는 두 방향 측정이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-140">A principled way of generalizing our methods to arbitrary number of classes is to replace qubits with qudits, i.e. quantum units with $d$ basis states, and the two-way measurement with $d$-way measurement.</span></span>

### <a name="likelihood-as-the-training-goal"></a><span data-ttu-id="2b281-141">학습 목표의 가능성</span><span class="sxs-lookup"><span data-stu-id="2b281-141">Likelihood as the training goal</span></span>

<span data-ttu-id="2b281-142">Learnable 퀀텀 회로 $U (\ 테타) $를 지정 하는 경우 $ \theta $는 매개 변수의 벡터이 고 $M $로 최종 측정을 나타내는 경우 올바른 레이블 유추의 평균 가능성은 $ $ \begin{align} \mathcal{L} (\theta) = \frac {1} {| \mathcal{D} |} \theta (\ sum_ {(x, y_1) \In\mathcal{D}} P (M = y_1 |)입니다. U (\theta) x) + \ sum_ {(x, y_2) \in\mathcal{D}} P (M = y_2 | U (\theta) x) \theta) \end{align} $ $ $P where (M = y | z) $는 퀀텀 상태에서 $y $를 측정할 확률 $z $입니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-142">Given a learnable quantum circuit $U(\theta)$, where $\theta$ is a vector of parameters, and denoting the final measurement by $M$, the average likelihood of the correct label inference is $$ \begin{align} \mathcal{L}(\theta)=\frac{1}{|\mathcal{D}|} \left( \sum_{(x,y_1)\in\mathcal{D}} P(M=y_1|U(\theta) x) + \sum_{(x,y_2)\in\mathcal{D}} P(M=y_2|U(\theta) x)\right) \end{align} $$ where $P(M=y|z)$ is the probability of measuring $y$ in quantum state $z$.</span></span>
<span data-ttu-id="2b281-143">여기서는 \mathcal{L} (\theta) $가 $ \theta $에서 부드러운 확률 함수 $ (\theta) $이 고 모든 $ \ theta_j $에서 파생 된 것을 기본적으로 확률 함수 자체를 계산 하는 데 사용 되는 것과 동일한 퀀텀 프로토콜을 사용 하 여 계산할 수 있음을 이해 해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-143">Here, it suffices to understand that the likelihood function $\mathcal{L}(\theta)$ is smooth in $\theta$ and its derivative in any $\theta_j$ can be computed by essentially the same quantum protocol as used for computing the likelihood function itself.</span></span> <span data-ttu-id="2b281-144">이렇게 하면 $ \mathcal{L} (\theta) $를 그라데이션 디센더로 최적화할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-144">This allows for optimizing the $\mathcal{L}(\theta)$ by gradient descent.</span></span>

### <a name="classifier-bias-and-training-score"></a><span data-ttu-id="2b281-145">분류자 편차 및 학습 점수</span><span class="sxs-lookup"><span data-stu-id="2b281-145">Classifier bias and training score</span></span>

<span data-ttu-id="2b281-146">$ \Theta $에서 매개 변수의 중간 (또는 최종) 값을 지정 하는 경우 유추를 수행 하기 위한 *분류자 바이어스* 로 지정 된 단일 실수 $b 값을 식별 해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-146">Given some intermediate (or final) values of the parameters in $\theta$, we need to identify a single real value $b$ know as *classifier bias* to do the inference.</span></span> <span data-ttu-id="2b281-147">레이블 유추 규칙은 다음과 같이 작동 합니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-147">The label inference rule works as follows:</span></span> 
- <span data-ttu-id="2b281-148">샘플 $x $에는 $P (M = y_2 | 인 경우에만 _2 $ if $y 레이블이 할당 됩니다. U (\theta) x) + b > $0.5 (RULE1) (그렇지 않으면 지정 된 레이블 $y _1 $)</span><span class="sxs-lookup"><span data-stu-id="2b281-148">A sample $x$ is assigned label $y_2$ if and only if $P(M=y_2|U(\theta) x) + b > 0.5$  (RULE1) (otherwise it is assigned label $y_1$)</span></span>

<span data-ttu-id="2b281-149">명확 하 게 $b $는 간격 $ (-0.5, + 0.5)에 있어야 합니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-149">Clearly $b$ must be in the interval $(-0.5,+0.5)$ to be meaningful.</span></span>

<span data-ttu-id="2b281-150">\Mathcal{D} $의 학습 사례 $ (x, y) \는 RULE1에 대 한 $x $에 대해 유추 된 레이블이 실제로는 $y $와 다른 경우에 대 한 잘못 된 $b *분류* 로 간주 됩니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-150">A training case $(x,y) \in \mathcal{D}$ is considered a *misclassification* given the bias $b$ if the label inferred for $x$ as per RULE1 is actually different from $y$.</span></span> <span data-ttu-id="2b281-151">전체 오 분류 수는 바이어스 $b $가 지정 된 경우 분류자의 *학습 점수* 입니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-151">The overall number of misclassifications is the *training score* of the classifier given the bias $b$.</span></span> <span data-ttu-id="2b281-152">*최적* 분류자 바이어스 $b $는 학습 점수를 최소화 합니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-152">The *optimal* classifier bias $b$ minimizes the training score.</span></span> <span data-ttu-id="2b281-153">사전 계산 확률 예측이 $ \{ P (M = y_2 |로 지정 된 경우이를 쉽게 확인할 수 있습니다. U (\theta) x) | (x, \*) \in\mathcal{D} \} $는 최대 $ \ log_2 (| \mathcal{D} |)를 만들어 간격 $ (-0.5, + 0.5)의 이진 검색에서 최적의 분류자 바이어스를 찾을 수 있습니다. $ 단계.</span><span class="sxs-lookup"><span data-stu-id="2b281-153">It is easy to see that, given the precomputed probability estimates $\{ P(M=y_2|U(\theta) x) | (x,\*)\in\mathcal{D} \}$, the optimal classifier bias can be found by binary search in interval $(-0.5,+0.5)$ by making at most $\log_2(|\mathcal{D}|)$ steps.</span></span>

### <a name="reference"></a><span data-ttu-id="2b281-154">참조</span><span class="sxs-lookup"><span data-stu-id="2b281-154">Reference</span></span>

<span data-ttu-id="2b281-155">이 정보는 코드로의 재생을 시작 하는 데 충분 해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="2b281-155">This information should be enough to start playing with the code.</span></span> <span data-ttu-id="2b281-156">그러나이 모델에 대 한 자세한 내용은 [ *' 회로 중심 퀀텀 분류자 ', 민 Schuld, Alex Bocharov, Krysta sva및 네 번째 wiebe* 의 원래 제안을 읽어 보세요.](https://arxiv.org/abs/1804.00633)</span><span class="sxs-lookup"><span data-stu-id="2b281-156">However, if you want to learn more about this model, please read the original proposal: [*'Circuit-centric quantum classifiers', Maria Schuld, Alex Bocharov, Krysta Svore and Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span></span>
